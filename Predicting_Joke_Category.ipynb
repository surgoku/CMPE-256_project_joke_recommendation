{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the Code for predicting the category of joke from the following categories:\n",
    "1. Animals\n",
    "2. Technology\n",
    "3. Doctor\n",
    "4. Main\n",
    "5. Politics\n",
    "6. Relationship\n",
    "7. Religion\n",
    "8. School\n",
    "9. Food\n",
    "10. Others\n",
    "\n",
    "\n",
    "We have pre-labelled the jokes data (150 jokes) into the above mentioned classes. Following is the distribution of the categoies:\n",
    "\n",
    "   8 animal\n",
    "   7 doctor\n",
    "   8 food\n",
    "   1 joke_category_reduced\n",
    "  11 man\n",
    "  58 others\n",
    "  15 politics\n",
    "  13 relationship\n",
    "   1 religion\n",
    "  10 school\n",
    "  19 technology\n",
    "\n",
    "\n",
    "Approach:\n",
    "=========\n",
    "\n",
    "Features for Jokes:\n",
    "1. Obtain key words for each joke\n",
    "2. Obtain Glove vectors (similar to Word2Vec) from pre-trained based on Wikipedia data of 300 dimension for each word and then averaging out for the entire joke.\n",
    "3. These averaged out 300 dimensional Glove vector for joke is used as feature for classification\n",
    "\n",
    "Classes:\n",
    "10 categories mentioned above.\n",
    "\n",
    "\n",
    "Training:\n",
    "==========\n",
    "The problem is posed as a multi-class classification problem with 10 classes. Since the data is of small size, only 10% of the jokes are used for testing while\n",
    "remaining 90% jokes are used for training. The following models were used:\n",
    "1. Gradient Boosting\n",
    "2. Naive Bayes\n",
    "3. SVM\n",
    "4. Logistic Regression\n",
    "5. Nearest Neighbors using Centroid\n",
    "6. K-nearest Neighbors\n",
    "7. Ensemble of all above classifiers\n",
    "\n",
    "Results:\n",
    "========\n",
    "Following are the results of various models:  \n",
    "\n",
    "\n",
    "Name: Gradient Boosting\n",
    "Accuracy score:  0.4\n",
    "\n",
    "Name: Naive Bayes\n",
    "Accuracy score:  0.466666666667\n",
    "\n",
    "Name: SVM\n",
    "Accuracy score:  0.533333333333\n",
    "\n",
    "Name: Logistic Regression\n",
    "Accuracy score:  0.533333333333\n",
    "\n",
    "Name: Nearest Neighbors using Centroid\n",
    "Accuracy score:  0.533333333333\n",
    "\n",
    "Name: K-nearest Neighbors\n",
    "Accuracy score:  0.466666666667\n",
    "\n",
    "Name: Ensemble\n",
    "Accuracy score:  0.466666666667\n",
    "\n",
    "\n",
    "Even though Nearest Neighbors and LR gives best performance, however they often bias towards \"others\" class. \n",
    "On qualitative analysis, Naive Bayes, Gradient boosting does better generally. KNN does better when their are enough samples for  a category, however it is biased towards \"others\" class which is a popular class making the acuracy to be high.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of models\n",
      "======================\n",
      "Name: Gradient Boosting\n",
      "Accuracy score:  0.4\n",
      "\n",
      "Name: Naive Bayes\n",
      "Accuracy score:  0.466666666667\n",
      "\n",
      "Name: SVM\n",
      "Accuracy score:  0.533333333333\n",
      "\n",
      "Name: Logistic Regression\n",
      "Accuracy score:  0.533333333333\n",
      "\n",
      "Name: Nearest Neighbors using Centroid\n",
      "Accuracy score:  0.533333333333\n",
      "\n",
      "Name: K-nearest Neighbors\n",
      "Accuracy score:  0.466666666667\n",
      "\n",
      "Name: Ensemble\n",
      "Accuracy score:  0.466666666667\n",
      "\n",
      "Predicting the category predicted by each model for each joke in test data: \n",
      "============================================================================\n",
      "\n",
      "Joke: \n",
      "\n",
      "Two kindergarten girls were talking outside: one said, \"You won't believe what I saw on the patio yesterday--a condom!\" The second girl asked, \"What's a patio?\" \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- man\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- school\n",
      "K-nearest Neighbors -- relationship\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "  An American tourist goes into a restaurant in Spain and orders the specialty of the house. When his dinner arrives, he asks the waiter what it is.      \"These, senor, \" replied the waiter in broken English,  \"are the testicles of the bull killed in the ring today. \"    The tourist swallowed hard but tasted the dish and thought it was delicious. So he comes back the next evening and orders the same item. When it is served, he says to the waiter,  \"These testicles...are much smaller than the ones I had last night. \"      \"Yes, senor, \" replied the waiter,  \"You see...the bull, he does not always lose.     \n",
      "\n",
      "True category --  food\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- food\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- food\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "A Czechoslovakian man felt his eyesight was growing steadily worse, and felt it was time to go see an optometrist. The doctor started with some simple testing, and showed him a standard eye chart with letters of diminishing size: CRKBNWXSKZY. . .  \"Can you read this?\" the doctor asked.  \"Read it?\" the Czech answered. \"Doc, I know him!\" \n",
      "\n",
      "True category --  doctor\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- man\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- doctor\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "A country guy goes into a city bar that has a dress code, and the maitre d' demands he wear a tie. Discouraged, the guy goes to his car to sulk when inspiration strikes: He's got jumper cables in the trunk! So he wraps them around his neck, sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d' is reluctant, but says to the guy, \"Okay, you're a pretty resourceful fellow, you can come in... but just don't start anything\"! \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- relationship\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "Q: Ever wonder why the IRS calls it Form 1040? A: Because for every $50 that you earn, you get 10 and they get 40. \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- technology\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "   Chuck Norris' calendar goes straight from March 31st to April 2nd; no one fools Chuck Norris.     \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "Q: How many programmers does it take to change a lightbulb? A: NONE! That's a hardware problem.... \n",
      "\n",
      "True category --  technology\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- technology\n",
      "K-nearest Neighbors -- technology\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "    \"Do you believe in life after death? \" the boss asked one of his employees.     \"Yes, sir, \" the new recruit replied.      \"Well, then, that makes everything just fine... \" the boss went on.      \"After you left early yesterday to go to your grandmother's funeral, she stopped in to see you. \"   \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- doctor\n",
      "Naive Bayes -- relationship\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- relationship\n",
      "K-nearest Neighbors -- relationship\n",
      "Ensemble -- relationship\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "Just a thought .. Before criticizing someone, walk a mile in their shoes.  Then when you do criticize them, you will be a mile away and have their shoes ! \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- relationship\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- man\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "At a recent Sacramento PC Users Group meeting, a company was demonstrating its latest speech- recognition software. A representative from the company was just about ready to start the demonstration and asked everyone in the room to quiet down. Just then someone in the back of the room yelled, \"Format C: Return.\"  Someone else chimed in: \"Yes, Return\"  Unfortunately, the software worked. \n",
      "\n",
      "True category --  technology\n",
      "Predicted categories: \n",
      "Gradient Boosting -- technology\n",
      "Naive Bayes -- technology\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- technology\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "   A drunk staggers into a Catholic Church, enters a confessional booth, sits down, but says nothing.   The Priest coughs a few times to get his attention but the drunk just sits there.     Finally, the Priest pounds three times on the wall.    The drunk mumbles,  \"Ain't no use knockin, there's no paper on this side either. \"     \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- man\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "   An American, a Scot and a Canadian were in a terrible car accident. They were all brought to the same emergency room, but all three of them died before they arrived.    Just as they were about to put the toe tag on the American, he stirred and opened his eyes. Astonished, the doctors and nurses present asked him what happened.     \"Well, \" said the American,  \"I remember the crash, and then there was a beautiful light, and then the Canadian and the Scot and I were standing at the gate s of heaven. St. Peter approached us and said that we were all to young to die, and that for a donation of $50, we could return to earth. So of course I pulled out my wallet and gave him the $50, and the next thing I knew I was back here. \"     \"That's amazing! \" said the one of the doctors,  \"But what happened to the other two? \"     \"Last I saw them, \" replied the American,  \"the Scot was haggling over the price and the Canadian was waiting for the government to pay his. \"     \n",
      "\n",
      "True category --  politics\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- school\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- food\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "Q. What's O. J. Simpson's Internet address? A. Slash, slash, backslash, slash, slash, escape. \n",
      "\n",
      "True category --  technology\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "  When most people claim to be  \"killing time \", it's only an expression. When Chuck Norris kills time, the minutes actually cease to exist.     \n",
      "\n",
      "True category --  others\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n",
      "Joke: \n",
      "\n",
      "Q: Do you know the difference between an intelligent male and the Sasquatch? A: There have been actual reported sightings of the Sasquatch. \n",
      "\n",
      "True category --  animal\n",
      "Predicted categories: \n",
      "Gradient Boosting -- others\n",
      "Naive Bayes -- others\n",
      "SVM -- others\n",
      "Logistic Regression -- others\n",
      "Nearest Neighbors using Centroid -- others\n",
      "K-nearest Neighbors -- others\n",
      "Ensemble -- others\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_key_words(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return continuous_chunk\n",
    "    \n",
    "\n",
    "def clean_joke(joke):\n",
    "    joke = re.sub(r'([^\\.\\s\\w]|_)+', '', joke).replace(\".\", \". \")\n",
    "    joke = \" \".join(extract_key_words(joke))\n",
    "    return joke\n",
    "\n",
    "def load_joke_classes_and_text():\n",
    "    data = pd.read_csv(\"../data/Jokes_labelling.txt\", delimiter=\"\\t\")\n",
    "    data['Jokes'] = data['Jokes'].map(lambda j: clean_joke(j))\n",
    "    data.drop('joke_category', axis=1, inplace=True)\n",
    "    cat_feats = pd.get_dummies(data['joke_category_reduced'], prefix='cat')\n",
    "    data = pd.concat([data['joke_id'], data['Jokes'], cat_feats], axis=1)\n",
    "    \n",
    "    Y = cat_feats\n",
    "    X = data['Jokes']\n",
    "    #print Y.describe()\n",
    "    #print X.describe()\n",
    "    \n",
    "def load_joke_classes_text_and_glove_vectors():\n",
    "    id_vectors = pd.read_csv(\"../data/Jokes_id_with_vectors.txt\", delimiter=\"\\t\")\n",
    "    id_vectors.drop(\"Unnamed: 301\", axis=1, inplace=True)\n",
    "    data = pd.read_csv(\"../data/Jokes_labelling.txt\", delimiter=\"\\t\")\n",
    "    cat_feats = pd.get_dummies(data['joke_category_reduced'], prefix='cat')\n",
    "    \n",
    "    Y = cat_feats\n",
    "    X = id_vectors\n",
    "    X['jokes'] = data['Jokes']\n",
    "    X['joke_category_reduced'] = data['joke_category_reduced']\n",
    "    #X = id_vectors.drop('joke_id', axis=1)\n",
    "    cols = X.columns.tolist()\n",
    "    cols.insert(1, cols.pop(cols.index('jokes')))\n",
    "    cols.insert(2, cols.pop(cols.index('joke_category_reduced')))\n",
    "    X = X.reindex(columns= cols)\n",
    "    \n",
    "    Y = data['joke_category_reduced']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def evaluting_and_predicting_joke_category_by_words(X,Y):\n",
    "    Y = Y.values\n",
    "    X = X.values\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    Y_c = le.fit_transform(Y)\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y_c, test_size=0.10, random_state=99)\n",
    "    \n",
    "    train_ids = X_train[:, 0]\n",
    "    train_jokes = X_train[:, 1]\n",
    "    test_ids = X_test[:, 0]\n",
    "    test_jokes = X_test[:, 1]\n",
    "    test_jokes_categ = X_test[:, 2]\n",
    "    \n",
    "    test_joke_true_categ_map = {}\n",
    "    for joke, true_categ in zip(test_jokes, test_jokes_categ):\n",
    "        test_joke_true_categ_map[joke] = true_categ\n",
    "        \n",
    "    X_train = X_train[:, 3:]\n",
    "    X_test = X_test[:, 3:]\n",
    "    \n",
    "    model_xgb = xgb.XGBClassifier()\n",
    "    model_nb = GaussianNB()\n",
    "    model_svm = svm.SVC()\n",
    "    model_lr = LogisticRegression()\n",
    "    model_knn_centroid = NearestCentroid()\n",
    "    model_knn = KNeighborsClassifier()\n",
    "    \n",
    "    eclf = VotingClassifier(estimators=[\n",
    "        ('lr', model_lr), ('knn_centroid', model_knn_centroid), ('gnb', model_nb), ('svc', model_svm), ('knn', model_knn), ('xgb', model_xgb)],\n",
    "        voting='hard', weights=[1,1,1,1,1,1])\n",
    "    \n",
    "    models = [model_xgb, model_nb, model_svm, model_lr, model_knn_centroid, model_knn, eclf]\n",
    "    model_names = [\"Gradient Boosting\", \"Naive Bayes\", \"SVM\", \"Logistic Regression\", \"Nearest Neighbors using Centroid\", \"K-nearest Neighbors\", \"Ensemble\"]\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    joke_predicts_by_model_map = {}\n",
    "    \n",
    "    print \"Performance of models\"\n",
    "    print \"======================\"\n",
    "    for model, name in zip(models, model_names):\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "        \n",
    "        print \"Name:\", name\n",
    "        print \"Accuracy score: \", accuracy\n",
    "        print \n",
    "        \n",
    "        best_preds = model.predict(X_test)\n",
    "\n",
    "        pred_categories = le.inverse_transform(best_preds)\n",
    "        categ_joke_map = {}\n",
    "        for id, categ, joke in zip(test_ids, pred_categories, test_jokes):\n",
    "            if categ not in categ_joke_map:\n",
    "                categ_joke_map[categ] = [joke]\n",
    "            else:\n",
    "                categ_joke_map[categ].append(joke)\n",
    "\n",
    "        \"\"\"\n",
    "        for categ in categ_joke_map:\n",
    "            print categ + \"\\n\" + \"======\"\n",
    "            for joke in categ_joke_map[categ]:\n",
    "                print joke\n",
    "                print \n",
    "            print \"\\n\\n\\n\"\n",
    "        \"\"\"\n",
    "        for categ in categ_joke_map:\n",
    "            for joke in categ_joke_map[categ]:\n",
    "                if joke not in joke_predicts_by_model_map:\n",
    "                    joke_predicts_by_model_map[joke] = []\n",
    "                joke_predicts_by_model_map[joke].append(categ)\n",
    "    \n",
    "    print \"Predicting the category predicted by each model for each joke in test data: \"\n",
    "    print \"============================================================================\\n\"\n",
    "    for joke in joke_predicts_by_model_map:\n",
    "        print \"Joke: \\n\" \n",
    "        print joke, \"\\n\"\n",
    "        print \"True category -- \", test_joke_true_categ_map[joke]\n",
    "        print \"Predicted categories: \"\n",
    "        for name, pred in zip(model_names, joke_predicts_by_model_map[joke]):\n",
    "            print name, \"--\" , pred\n",
    "        #print \"Models:\"\n",
    "        #print model_names\n",
    "        #print \"predicted categories:\"\n",
    "        #print joke_predicts_by_model_map[joke]\n",
    "        print \"\\n\\n\"\n",
    "        \n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    X, Y = load_joke_classes_text_and_glove_vectors()\n",
    "    evaluting_and_predicting_joke_category_by_words(X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
